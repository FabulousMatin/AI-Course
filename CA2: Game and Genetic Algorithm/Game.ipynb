{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game: Minimax Search\n",
    "\n",
    "we are going to decide best move in Othello game by runnig minimax algorithm, then we are going to apply alpha-beta pruning and see its effect on time and seen states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import turtle\n",
    "from copy import deepcopy\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN  = 1\n",
    "CPU  = -1\n",
    "\n",
    "DEPTH_LEAF = 2\n",
    "ENDGAME_LEAF = 3\n",
    "\n",
    "INF = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloUI:\n",
    "    def __init__(self, board_size=6, square_size=60):\n",
    "        self.board_size = board_size\n",
    "        self.square_size = square_size\n",
    "        self.screen = turtle.Screen()\n",
    "        self.screen.setup(self.board_size * self.square_size + 50, self.board_size * self.square_size + 50)\n",
    "        self.screen.bgcolor('white')\n",
    "        self.screen.title('Othello')\n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.hideturtle()\n",
    "        self.pen.speed(0)\n",
    "        turtle.tracer(0, 0)\n",
    "\n",
    "    def draw_board(self, board):\n",
    "        self.pen.penup()\n",
    "        x, y = -self.board_size / 2 * self.square_size, self.board_size / 2 * self.square_size\n",
    "        for i in range(self.board_size):\n",
    "            self.pen.penup()\n",
    "            for j in range(self.board_size):\n",
    "                self.pen.goto(x + j * self.square_size, y - i * self.square_size)\n",
    "                self.pen.pendown()\n",
    "                self.pen.fillcolor('green')\n",
    "                self.pen.begin_fill()\n",
    "                self.pen.setheading(0)\n",
    "                for _ in range(4):\n",
    "                    self.pen.forward(self.square_size)\n",
    "                    self.pen.right(90)\n",
    "                self.pen.penup()\n",
    "                self.pen.end_fill()\n",
    "                self.pen.goto(x + j * self.square_size + self.square_size / 2,\n",
    "                              y - i * self.square_size - self.square_size + 5)\n",
    "                if board[i][j] == 1:\n",
    "                    self.pen.fillcolor('white')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "                elif board[i][j] == -1:\n",
    "                    self.pen.fillcolor('black')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "\n",
    "        turtle.update()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All changes:\n",
    "- get_valid_moves and make_move methods now works with given board if no board given, default game board(currently playing board) will be used.\n",
    "- A nested Minimax class added, encapsulating every state's informations in a class and using it to access children and parents can be helpful\n",
    "\n",
    "Q1 : calc_eval calculate evaluation of a leaf, we have 2 kind of leaves, depth-limited leaves and end-game leaves, evaluation of an end-game leaf must be so great or so low, because the game is finished and we got the winner thus it has more proirity comparing to depth-limited leaves, when we reach limit of depth we produce a depth-limited leaf which calculate evaluation by subtracting numbers of discs, a good act can be giving weight to board corners and edges, corner discs can not be changed and edge discs are game-changing in Othello, as we analyse minimax algoritm, AI decides edge and corner discs however it results in less disc changes, but in deeper depthes it has a really good affect to change huge amount of discs' color. \n",
    "\n",
    "Notes:\n",
    "- if there is no more valid moves, that state is end-game leaf, and has a great effect on evaluation.\n",
    "- end-game leaf does not affects normally in game tie\n",
    "- every state updates its parent and children, saving them can be helpful in further analyses.\n",
    "- alpha and beta is calculated for every state, I using simple a >= b condition instead of recursive way.\n",
    "- prune is conditionally, both way will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Othello:\n",
    "    def __init__(self, ui, minimax_depth=1, prune=True):\n",
    "        self.size = 6\n",
    "        self.ui = OthelloUI(self.size) if ui else None\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2) - 1] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2)] = 1\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2)] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2) - 1] = -1\n",
    "        self.current_turn = random.choice([1, -1])\n",
    "        self.minimax_depth = minimax_depth\n",
    "        self.prune = prune\n",
    "\n",
    "    def get_winner(self):\n",
    "        white_count = sum([row.count(1) for row in self.board])\n",
    "        black_count = sum([row.count(-1) for row in self.board])\n",
    "        if white_count > black_count:\n",
    "            return 1\n",
    "        elif white_count < black_count:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_valid_moves(self, player, board = None):\n",
    "        if board == None:\n",
    "            board = self.board\n",
    "\n",
    "        moves = set()\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if board[i][j] == 0:\n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "                            x, y = i, j\n",
    "                            captured = []\n",
    "                            while 0 <= x + di < self.size and 0 <= y + dj < self.size and board[x + di][\n",
    "                                    y + dj] == -player:\n",
    "                                captured.append((x + di, y + dj))\n",
    "                                x += di\n",
    "                                y += dj\n",
    "                            if 0 <= x + di < self.size and 0 <= y + dj < self.size and board[x + di][\n",
    "                                    y + dj] == player and len(captured) > 0:\n",
    "                                moves.add((i, j))\n",
    "        return list(moves)\n",
    "\n",
    "\n",
    "    def make_move(self, player, move, board = None):\n",
    "        if board == None:\n",
    "            board = self.board\n",
    "\n",
    "        i, j = move\n",
    "        board[i][j] = player\n",
    "        for di in [-1, 0, 1]:\n",
    "            for dj in [-1, 0, 1]:\n",
    "                if di == 0 and dj == 0:\n",
    "                    continue\n",
    "                x, y = i, j\n",
    "                captured = []\n",
    "                while 0 <= x + di < self.size and 0 <= y + dj < self.size and board[x + di][y + dj] == -player:\n",
    "                    captured.append((x + di, y + dj))\n",
    "                    x += di\n",
    "                    y += dj\n",
    "                if 0 <= x + di < self.size and 0 <= y + dj < self.size and board[x + di][y + dj] == player:\n",
    "                    for (cx, cy) in captured:\n",
    "                        board[cx][cy] = player\n",
    "\n",
    "    def get_cpu_move(self):\n",
    "        moves = self.get_valid_moves(-1)\n",
    "        if len(moves) == 0:\n",
    "            return None\n",
    "        return random.choice(moves)\n",
    "\n",
    "    class Minimax:\n",
    "        def __init__(self, board, player, parent, prev_move, next_move, alpha, beta):\n",
    "            self.board = board\n",
    "            self.player = player\n",
    "            self.parent = parent\n",
    "            self.prev_move = prev_move\n",
    "            self.next_move = next_move\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.children = []\n",
    "\n",
    "            if(player == HUMAN):\n",
    "                self.evaluation = -INF\n",
    "            elif(player == CPU):\n",
    "                self.evaluation = INF\n",
    "\n",
    "        def __lt__(self, other):\n",
    "            return self.evaluation < other.evaluation\n",
    "\n",
    "        def calc_eval(self, leaf_type):\n",
    "            diff = sum([row.count(1) for row in self.board]) - sum([row.count(-1) for row in self.board])\n",
    "            if(leaf_type == DEPTH_LEAF or diff == 0):\n",
    "                self.evaluation = diff\n",
    "                self.alpha = diff\n",
    "                self.beta = diff\n",
    "            elif(diff < 0):\n",
    "                self.evaluation = -36\n",
    "                self.alpha = -36\n",
    "                self.beta = -36\n",
    "            elif(diff > 0):\n",
    "                self.evaluation = 36\n",
    "                self.alpha = 36\n",
    "                self.beta = 36\n",
    "            \n",
    "        \n",
    "        def pruning(self):\n",
    "            return(self.alpha >= self.beta)\n",
    "\n",
    "        def handle_parent(self, leaf_type = 0):\n",
    "            curr = self\n",
    "            par = self.parent\n",
    "\n",
    "            if(leaf_type != 0):\n",
    "                self.calc_eval(leaf_type)\n",
    "\n",
    "            if par == None:\n",
    "                return\n",
    "\n",
    "            if(par.player == HUMAN and curr.evaluation > par.evaluation):\n",
    "                par.evaluation = curr.evaluation\n",
    "                par.next_move = curr.prev_move\n",
    "                par.alpha = max(par.alpha, par.evaluation)\n",
    "            elif(par.player == CPU and curr.evaluation < par.evaluation):\n",
    "                par.evaluation = curr.evaluation\n",
    "                par.next_move = curr.prev_move\n",
    "                par.beta = min(par.beta, par.evaluation)        \n",
    "        \n",
    "\n",
    "    def minimax_search(self, curr_state, depth):\n",
    "        \n",
    "        if(depth < self.minimax_depth):\n",
    "            moves = self.get_valid_moves(curr_state.player, curr_state.board)\n",
    "            if not moves:\n",
    "                curr_state.player = -curr_state.player\n",
    "                curr_state.evaluation = -curr_state.evaluation\n",
    "                moves = self.get_valid_moves(curr_state.player, curr_state.board)\n",
    "                if not moves:\n",
    "                    curr_state.handle_parent(ENDGAME_LEAF)\n",
    "                    return\n",
    "\n",
    "            for move in moves:\n",
    "                \n",
    "                if(self.prune):\n",
    "                    if(curr_state.pruning()):\n",
    "                        return\n",
    "\n",
    "                new_board = deepcopy(curr_state.board)\n",
    "                self.make_move(curr_state.player, move, new_board)\n",
    "                new_state = self.Minimax(new_board, -curr_state.player, curr_state, move, None, curr_state.alpha, curr_state.beta)\n",
    "                curr_state.children.append(new_state)\n",
    "\n",
    "                self.minimax_search(new_state, depth + 1)\n",
    "            curr_state.handle_parent()\n",
    "            \n",
    "        else:\n",
    "            curr_state.handle_parent(DEPTH_LEAF)\n",
    "\n",
    "    def get_human_move(self):\n",
    "        curr = self.Minimax(deepcopy(self.board), self.current_turn, None, None, None, -INF, INF)\n",
    "        self.minimax_search(curr, 0)\n",
    "        return curr.next_move\n",
    "        \n",
    "    def terminal_test(self):\n",
    "        return len(self.get_valid_moves(1)) == 0 and len(self.get_valid_moves(-1)) == 0\n",
    "\n",
    "    def play(self):\n",
    "        winner = None\n",
    "        while not self.terminal_test():\n",
    "            if self.ui:\n",
    "                self.ui.draw_board(self.board)\n",
    "            if self.current_turn == 1:\n",
    "                move = self.get_human_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            else:\n",
    "                move = self.get_cpu_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            self.current_turn = -self.current_turn\n",
    "            if self.ui:\n",
    "                self.ui.draw_board(self.board)\n",
    "                time.sleep(2)\n",
    "        winner = self.get_winner()\n",
    "        return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pruning results:\n",
      "\n",
      "depth =  1\n",
      "win_rate = 76.66666666666667\n",
      "average game time =  0.006697195833333088 \n",
      "\n",
      "depth =  3\n",
      "win_rate = 82.5\n",
      "average game time =  0.14107655249999976 \n",
      "\n",
      "depth =  5\n",
      "win_rate = 99.16666666666667\n",
      "average game time =  5.703711637500001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('without pruning results:\\n')\n",
    "times = 120\n",
    "depthes = [1,3,5]\n",
    "for depth in depthes:\n",
    "    wins = 0\n",
    "    print('depth = ',depth)\n",
    "    start = timer()\n",
    "    for i in range(times):\n",
    "        game = Othello(ui=False, minimax_depth=depth, prune=False)\n",
    "        if(game.play() == 1):\n",
    "            wins += 1\n",
    "        del game\n",
    "    end = timer()\n",
    "    print('win_rate =', (wins/times) * 100)\n",
    "    print('average game time = ',(end - start)/ times,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with pruning results:\n",
      "\n",
      "depth =  1\n",
      "win_rate = 73.33333333333333\n",
      "average game time =  0.006702328333333677 \n",
      "\n",
      "depth =  3\n",
      "win_rate = 86.66666666666667\n",
      "average game time =  0.06207985583333387 \n",
      "\n",
      "depth =  5\n",
      "win_rate = 91.66666666666666\n",
      "average game time =  0.6425725116666664 \n",
      "\n",
      "depth =  7\n",
      "win_rate = 99.16666666666667\n",
      "average game time =  7.245271075 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('with pruning results:\\n')\n",
    "times = 120\n",
    "depthes = [1,3,5,7]\n",
    "for depth in depthes:\n",
    "    wins = 0\n",
    "    print('depth = ',depth)\n",
    "    start = timer()\n",
    "    for i in range(times):\n",
    "        game = Othello(ui=False, minimax_depth=depth, prune=True)\n",
    "        if(game.play() == 1):\n",
    "            wins += 1\n",
    "        del game\n",
    "    end = timer()\n",
    "    print('win_rate =', (wins/times) * 100)\n",
    "    print('average game time = ',(end - start)/ times,'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: as the depth increases, winning rate and average game time increases. in higher depthes we must analyse more states and game tree will be more complicated, but we predict more moves and choose a better decide which causes more wins.\n",
    "\n",
    "Q3: some kind of acts can be done to have more pruning, like iterate more high-amount max nodes and less-amount min nodes first, but that causes an overload to program which may reduce efficiency.\n",
    "\n",
    "Q4: every allowed moves and its effect to board and discs is a branching factor, first we have limited moves while in next moves we have more decisions, but from second half of game decisions gets less, by analysing children of nodes I guess it has a normal distribution.\n",
    "\n",
    "Q5: pruning cuts no more useful children which analysing them does not change answer of minimax search, imagine we have a min father in our path to a node if we assign a higher amount of that node to a max node, the other children of that nodes are no more longer helpful, because we already found a bigger amount sibbling.(vice versa)\n",
    "\n",
    "Q6: if the opponent does not take best action, we can use expectimax algorithm insted. using minimax against a random-based(or not completely efficient) opponent can prevent us to reach a higher amount leaf, according the fact we found best way in worst-case, but in avg-case we can reach a better score.\n",
    "\n",
    "in this case which opponent is completely random we can use uniform distribution and give sum of 1/allowed_moves * child.evaluation to a node as its evaluation isntead of minimum evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
