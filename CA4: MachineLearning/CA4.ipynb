{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from seaborn import heatmap, diverging_palette\n",
    "import matplotlib.colors\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import category_encoders as enc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Analyze and preproccessing\n",
    "Home sales prices and characteristics for Seattle and King County, WA (May 2014 - 2015) and its corresponding shape file with the zip code zones for King County.\n",
    "\n",
    "Observations = 21,613\n",
    "\n",
    "- id =\tIdentification\\\n",
    "- date =\tDate sold\\\n",
    "- price =\tSale price\\\n",
    "- bedrooms =\tNumber of bedrooms\\\n",
    "- bathrooms =\tNumber of bathrooms\\\n",
    "- sqft_liv =\tSize of living area in square feet\\\n",
    "- sqft_lot =\tSize of the lot in square feet\\\n",
    "- floors = Number of floors\\\n",
    "- waterfront =\t‘1’ if the property has a waterfront, ‘0’ if not.\\\n",
    "- view =\tAn index from 0 to 4 of how good the view of the property was\\\n",
    "- condition =\tCondition of the house, ranked from 1 to 5\\\n",
    "- grade =\tClassification by construction quality which refers to the types of materials used and the quality of workmanship. Buildings of better quality (higher grade) cost more to build per unit of measure and command higher value. Additional information in: KingCounty\n",
    "- sqft_above =\tSquare feet above ground\\\n",
    "- sqft_basmt =\tSquare feet below ground\\\n",
    "- yr_built =\tYear built\\\n",
    "- yr_renov =\tYear renovated. ‘0’ if never renovated\\\n",
    "- zipcode =\t5 digit zip code\\\n",
    "- lat =\tLatitude\\\n",
    "- long =\tLongitude\\\n",
    "- squft_liv15 =\tAverage size of interior housing living space for the closest 15 houses, in square feet\\\n",
    "- squft_lot15 =\tAverage size of land lots for the closest 15 houses, in square feet\n",
    "\n",
    "\n",
    "`goal` of this project is `predicting price` of a house by analyzing its other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.primitive = deepcopy(data)\n",
    "        self.data = data\n",
    "\n",
    "    def reset_data(self):\n",
    "        self.data = self.primitive\n",
    "        return self.data\n",
    "\n",
    "    def get_missing(self):\n",
    "        nan_values = pd.DataFrame()\n",
    "        nan_values['missing_count'] = self.data.isna().sum()\n",
    "        nan_values['missing_percentage'] = nan_values['missing_count'] / len(self.data)\n",
    "\n",
    "        return nan_values.round(4)\n",
    "\n",
    "    def fill_nan(self):\n",
    "        self.data.fillna(self.data.median(), inplace=True)\n",
    "    \n",
    "    def drop_unused(self, to_drop: list):\n",
    "        self.data.drop(to_drop, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    def get_neg_cols(self):\n",
    "        numeric_cols = self.data.select_dtypes(include='number').min()\n",
    "        neg_min = numeric_cols[numeric_cols < 0].index.tolist()\n",
    "        return neg_min\n",
    "        \n",
    "\n",
    "    def normalize(self, exclude_cols):\n",
    "        numeric = self.data.select_dtypes(include='number')\n",
    "        self.data[numeric.columns] = MinMaxScaler().fit_transform(numeric)\n",
    "        self.data[exclude_cols] = numeric[exclude_cols]\n",
    "\n",
    "    def encode_categories(self):\n",
    "        to_encode = self.data.select_dtypes(include=['category', 'object'])\n",
    "        self.data[to_encode.columns] = to_encode.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data info\n",
    "as we see there is 3 categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('house_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Missing data\n",
    "there is about 3000 rows with missing values, so droping them(14% of data!) is not a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcess(data)\n",
    "pre.get_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove unneccery data\n",
    "as we see first three columns are completely useless, so we drop them.\n",
    "\n",
    "plus, we do not need house IDs, it is unique and does not provide analyzable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.drop_unused(['Unnamed: 0.1', 'Unnamed: 0', 'Unnamed: 0.1.1', 'id'])\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Correlation\n",
    "\n",
    "Correlation is a statistical measure that describes the relationship between two variables. It measures the degree to which two variables are related to each other, and the direction of that relationship.\n",
    "\n",
    "Correlation can be positive, negative, or zero. A positive correlation means that as one variable increases, the other variable also tends to increase. A negative correlation means that as one variable increases, the other variable tends to decrease. A zero correlation means that there is no relationship between the two variables.\n",
    "\n",
    "The strength of the correlation is measured by the correlation coefficient, which is a value between -1 and 1. A correlation coefficient of -1 indicates a perfect negative correlation, a correlation coefficient of 0 indicates no correlation, and a correlation coefficient of 1 indicates a perfect positive correlation.\n",
    "\n",
    "\n",
    "$$corr(x,y) = \\frac{n\\sum\\limits_{i=1}^n x_i y_i - \\sum\\limits_{i=1}^n x_i \\sum\\limits_{i=1}^n y_i}{\\sqrt{(n\\sum\\limits_{i=1}^n x_i^2 - (\\sum\\limits_{i=1}^n x_i)^2)(n\\sum\\limits_{i=1}^n y_i^2 - (\\sum\\limits_{i=1}^n y_i)^2)}}$$\n",
    "\n",
    "- n is the number of observations\n",
    "- x is the first variable\n",
    "- y is the second variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "cmap = diverging_palette(220, 220, as_cmap=True)\n",
    "heatmap(corr, annot=True, fmt=\".3f\", cmap = cmap, linewidths=1, square=True, center=0, vmax=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr[['price']]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "cmap = diverging_palette(220, 220, as_cmap=True)\n",
    "heatmap(corr, annot=True, fmt=\".3f\", cmap = cmap, linewidths=1, square=True, center=0, vmax=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if(col == 'price'):\n",
    "        continue\n",
    "\n",
    "    print(col, ':')\n",
    "\n",
    "    if(data[col].dtype != 'object'):\n",
    "        corr = data[col].corr(data['price'])\n",
    "        print(f\"corr = {corr}\")\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20,8))\n",
    "        axs[1].hexbin(x = data[col], y = data['price'], gridsize = 15, cmap='Blues')\n",
    "    else:\n",
    "        fig, axs[0] = plt.subplots(1, 1, figsize=(8,8))\n",
    "        \n",
    "    axs[0].scatter(data[col], data['price'])\n",
    "    axs[0].set_title(\"Correlation between \" + col + \" and price\")\n",
    "    axs[0].set_xlabel(col)\n",
    "    axs[0].set_ylabel(\"price\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 removing noises and unmeaning values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is some negative values, only `long` feature can be negative.\n",
    "\n",
    "According to datas about house, floor can be 1.5 and it make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cols = pre.get_neg_cols()\n",
    "\n",
    "neg_cols.remove('long')\n",
    "negative_rows = data[data.loc[:, neg_cols] < 0].dropna(how='all')\n",
    "\n",
    "display(negative_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is only 26 data with illegal negative values, so we drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(negative_rows.index, axis = 0, inplace=True)\n",
    "pre.get_neg_cols()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Detect and analyze high correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_CUTOFF = 0.1\n",
    "\n",
    "high_corr = data.corr()['price'].drop(['price'])\n",
    "high_corr = high_corr[abs(high_corr) > CORR_CUTOFF]\n",
    "print(high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in high_corr.index:\n",
    "    plt.hist(data[feature], edgecolor='white')\n",
    "    tick = (data[feature].max() - data[feature].min()) // 15\n",
    "    tick = tick if tick > 0 else 1\n",
    "    plt.xticks(np.arange(data[feature].min(), data[feature].max() + 1, tick), rotation = 45)\n",
    "    plt.ylabel('occurances')\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Handle Missing Values\n",
    "\n",
    "we don't have missing for categorical features, so we use `median` method to fill them, some of methods are:\n",
    "\n",
    "- `fillna()`: This method is used to fill missing values with a specified value or method. For example, you can fill missing values with a constant value like 0 or a method like forward fill or backward fill.\n",
    "\n",
    "- `dropna()`: This method is used to remove rows or columns that contain missing values. This can be useful if you have a large dataset and only a small percentage of the data is missing.\n",
    "\n",
    "- `interpolate()`: This method is used to fill missing values with interpolated values. Interpolation is a method of estimating missing values based on the values of neighboring data points.\n",
    "\n",
    "- `Imputation`: This method involves using statistical methods to estimate missing values based on the values of other variables in the dataset. There are several imputation methods available in pandas, such as `mean` imputation, `median` imputation, and `regression` imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include='object').isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.fill_nan()\n",
    "pre.get_missing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Encoding Categorical Values\n",
    "\n",
    "- `Label Encoding`: This method assigns a unique integer label to each category in the data. This is useful when the categories have an inherent order or ranking, such as low, medium, and high. The LabelEncoder class from the sklearn.preprocessing module can be used to perform label encoding.\n",
    "\n",
    "- `One-Hot Encoding`: This method creates a binary column for each category in the data, indicating whether the category is present or not. This is useful when the categories do not have an inherent order or ranking, and when the number of categories is small. The get_dummies() function from Pandas can be used to perform one-hot encoding.\n",
    "\n",
    "- `Binary Encoding`: This method creates binary columns for each category in the data, but instead of using a single binary column for each category, it uses a binary code for each category. This is useful when the number of categories is large, as it reduces the number of columns needed for encoding. The BinaryEncoder class from the category_encoders library can be used to perform binary encoding.\n",
    "\n",
    "- `Count Encoding`: This method replaces each category in the data with the number of times it appears in the dataset. This is useful when the frequency of each category is important, and when the number of categories is large. The CountEncoder class from the category_encoders library can be used to perform count encoding.\n",
    "\n",
    "- `Target Encoding`: This method replaces each category in the data with the mean of the target variable for that category. This is useful when the relationship between the categories and the target variable is important, and when the number of categories is large. The TargetEncoder class from the category_encoders library can be used to perform target encoding.\n",
    "\n",
    "\n",
    "for this case, we choose label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.encode_categories()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.9 Data Scaling\n",
    "\n",
    "`Normalizing`: Normalizing is the process of scaling the values of a column to a range between 0 and 1. This is done by subtracting the minimum value of the column from each value, and then dividing by the range of the column (i.e., the difference between the maximum and minimum values). Normalizing is useful when the absolute values of the data are not important, but the relative values are.\n",
    "\n",
    "`Standardizing`: Standardizing is the process of scaling the values of a column to have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of the column from each value, and then dividing by the standard deviation of the column. Standardizing is useful when the absolute values of the data are important, and when the data has a normal distribution.\n",
    "\n",
    "why do we need them:\n",
    "\n",
    "- `Different scales`: The features in a DataFrame may have different scales, which can cause some features to dominate the others in the analysis. For example, if one feature has values in the range of 0-1 and another feature has values in the range of 0-1000, the second feature will have a much larger impact on the analysis. Normalizing or standardizing the data can ensure that all features have a similar scale and prevent this issue.\n",
    "\n",
    "- `Outliers`: Outliers in the data can also have a significant impact on the analysis. Normalizing or standardizing the data can reduce the impact of outliers by scaling the data to a more reasonable range.\n",
    "\n",
    "- `Algorithm requirements`: Some machine learning algorithms require the data to be normalized or standardized in order to work properly. For example, algorithms that use distance measures, such as k-nearest neighbors or clustering algorithms, can be sensitive to the scale of the data. Normalizing or standardizing the data can ensure that these algorithms work properly.\n",
    "\n",
    "- `Interpretability`: Normalizing or standardizing the data can make the results of the analysis more interpretable. For example, if the data is standardized to have a mean of 0 and a standard deviation of 1, the coefficients of a linear regression model will represent the change in the response variable for a one standard deviation change in the predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.normalize(['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Validation Data\n",
    "\n",
    "Validation in data refers to the process of evaluating the performance of a machine learning model on a dataset that is separate from the training dataset. The purpose of validation is to estimate how well the model will perform on new, unseen data.\n",
    "There are several types of validation techniques that can be used in machine learning, including:\n",
    "\n",
    "- `Holdout validation`: This technique involves splitting the dataset into two parts: a training set and a validation set. The model is trained on the training set and evaluated on the validation set. The performance on the validation set is used to estimate the performance on new, unseen data.\n",
    "\n",
    "- `Cross-validation`: This technique involves splitting the dataset into k-folds, where k is a user-defined parameter. The model is trained on k-1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold used as the validation set once. The performance on the k validation sets is averaged to estimate the performance on new, unseen data.\n",
    "\n",
    "- `Leave-one-out validation`: This technique involves using all but one data point for training and the remaining data point for validation. This process is repeated for each data point in the dataset. The performance on the validation sets is averaged to estimate the performance on new, unseen data.\n",
    "\n",
    "We perfer not having validation in this project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Train and Test data\n",
    "\n",
    "- `The training set` is a subset of the available data that is used to train the machine learning model. The model is trained on the input features (also known as predictors or independent variables) and the corresponding output values (also known as labels or dependent variables) in the training set. The goal of training is to learn a mapping between the input features and the output values, so that the model can make accurate predictions on new, unseen data.\n",
    "\n",
    "- `The test set` is a subset of the available data that is used to evaluate the performance of the machine learning model. The model is applied to the input features in the test set to make predictions, and the predicted output values are compared to the actual output values in the test set. The performance of the model is then evaluated using various metrics, such as accuracy, precision, recall, F1 score, etc.\n",
    "\n",
    "\n",
    "I perfer to have 10% test and 90% train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None \n",
    "        self.y_test = None\n",
    "\n",
    "    def split_for_classification(self, goal):\n",
    "        splitted = train_test_split(self.data.drop([goal], axis = 1), self.data[goal], test_size = 0.1, random_state=1)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = splitted\n",
    "        \n",
    "    def split_for_linearReg(self, x, y):\n",
    "        splitted = train_test_split(self.data[x], self.data[y], test_size = 0.1, random_state=1)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = splitted\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "Main form of simple linear regression function: \n",
    "$$f(x) = \\alpha x + \\beta$$\n",
    "\n",
    "here we want to find the intercept($\\alpha$) and slope($\\beta$) by minimizing the derivation of the RSS function:\n",
    "\n",
    "- step 1: Compute RSS of the training data  \n",
    "\n",
    "$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n",
    "\n",
    "- step 2: Compute the derivatives of the RSS function in term of $\\underline{\\alpha}$ and $\\underline{\\beta}$, and set them equal to 0 to find the desired parameters\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n",
    "$$ \\to \\hat{\\beta} = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n",
    "\n",
    "$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n",
    "$$ \n",
    "$$ \\hat{\\beta} = \\hat{y} - \\hat{\\alpha} \\hat{x}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `RMSE`:(Root Mean Squared Error) RMSE measures the average distance between the predicted values and the actual values in a dataset. It is calculated as the square root of the average of the squared differences between the predicted and actual values. RMSE is a measure of the spread of the residuals, which are the differences between the predicted values and the actual values. A lower RMSE indicates that the model is better at predicting the actual values.\n",
    "\n",
    "- `R2 score`: R2 score measures the proportion of the variance in the dependent variable that is explained by the independent variables. It is calculated as 1 minus the ratio of the residual sum of squares (RSS) to the total sum of squares (TSS). R2 score ranges from 0 to 1, with a higher score indicating a better fit of the model to the data.\n",
    "\n",
    "- `RSS`:(Residual Sum of Squares) RSS measures the sum of the squared differences between the predicted values and the actual values. It is calculated as the sum of the squared residuals. RSS is a measure of the amount of unexplained variance in the dependent variable.\n",
    "\n",
    "- `MSE`:(Mean Squared Error) MSE measures the average of the squared differences between the predicted values and the actual values. It is calculated as the average of the squared residuals. MSE is a measure of the spread of the residuals, similar to RMSE.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i,true} - y_{i,pred})^2}$$\n",
    "\n",
    "where $y_{i,pred}$ is the predicted value, $y_{i,true} $ is the actual value, and mean is the average over all the samples in the dataset.\n",
    "\n",
    "\n",
    "AS you might guessed, the RMSE has no bound and it is not easy to find out the percentage of fitting the model into data with it. instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. Matematically, the R2 score formula is shown as follows:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSres}{SStot} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\\sum_{i=1}^{n} (y_{i,true} - \\bar{y}_{true})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_regression():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.intercept = ((self.data.X_train - self.data.X_train.mean()) * (self.data.y_train - self.data.y_train.mean())).sum() / ((self.data.X_train - self.data.X_train.mean())**2).sum()\n",
    "        self.slope = self.data.y_train.mean() - self.intercept * self.data.X_train.mean()\n",
    "        self.predicted_values = self.intercept * self.data.X_test + self.slope\n",
    "        self.RMSE = None\n",
    "        self.R2 = None\n",
    "\n",
    "    def get_intercept_slope(self):\n",
    "        return(self.intercept, self.slope)\n",
    "    \n",
    "    def get_predictions(self):\n",
    "        return(self.predicted_values)\n",
    "    \n",
    "    def get_RSME(self):\n",
    "        n = self.predicted_values.shape[0]\n",
    "        self.RMSE = math.sqrt(((self.predicted_values - self.data.y_test) ** 2).sum() / n)\n",
    "        return(self.RMSE)\n",
    "\n",
    "    def get_RSME_train(self):\n",
    "        train_predict = self.intercept * self.data.X_train + self.slope\n",
    "        n = train_predict.shape[0]\n",
    "        train_RMSE = math.sqrt(((train_predict - self.data.y_train) ** 2).sum() / n)\n",
    "        return(train_RMSE)\n",
    "\n",
    "    def get_R2_score(self):\n",
    "        self.R2 = 1 - (((self.predicted_values - self.data.y_test) ** 2).sum() / ((self.data.y_test - self.data.y_test.mean()) ** 2).sum())\n",
    "        return(self.R2)\n",
    "\n",
    "    def get_R2_score_train(self):\n",
    "        train_predict = self.intercept * self.data.X_train + self.slope\n",
    "        train_R2 = 1 - (((train_predict - self.data.y_train) ** 2).sum() / ((self.data.y_train - self.data.y_train.mean()) ** 2).sum())\n",
    "        return(train_R2)\n",
    "    \n",
    "    def show_result(self):\n",
    "        print(feature, ':')\n",
    "        a, b = self.get_intercept_slope()\n",
    "        print('RSME =', self.get_RSME())\n",
    "        print('RSME train =', self.get_RSME_train())\n",
    "        print('R2 =', self.get_R2_score())\n",
    "        print('R2 train =', self.get_R2_score_train())\n",
    "        x = np.linspace(0,1,100)\n",
    "        y = a * x + b\n",
    "        plt.plot(x, y, color = 'red')\n",
    "        plt.scatter(self.data.X_train, self.data.y_train)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set(high_corr.index.to_list() + ['sqft_living' , 'yr_built' , 'grade' , 'zipcode'])\n",
    "goal = 'price'\n",
    "prepeared_data = Data(data)\n",
    "\n",
    "for feature in features:\n",
    "    prepeared_data.split_for_linearReg(feature, goal)\n",
    "    reg = Linear_regression(prepeared_data)\n",
    "    reg.show_result()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Analyze\n",
    "\n",
    "this is a `linear` regression, we can model our learning on a single feature, so blue dots is our data and red line is prediction, according to above statistics and R2 score :\n",
    "- linear regression is not proper for this case, because of having many effecting variables\n",
    "- `zipcode` and `yr_built` are completely unlinear\n",
    "- `sqft_living` and `grade` having slightly linear effect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "- `Decision Tree`: A decision tree is a type of supervised learning algorithm that is mostly used for classification problems. It works by recursively splitting the data into subsets based on the values of the input features, until a stopping criterion is met. The result is a tree-like structure where each internal node represents a test on an input feature, each branch represents the outcome of the test, and each leaf node represents a class label. Decision trees are easy to interpret and can handle both categorical and numerical data.\n",
    "\n",
    "- `K-Nearest Neighbor`: K-Nearest Neighbor (KNN) is a type of supervised learning algorithm that is mostly used for classification problems. It works by finding the k nearest neighbors to a given data point in the training set, and assigning the class label that is most common among the k neighbors to the data point. KNN is a non-parametric algorithm, which means that it does not make any assumptions about the underlying distribution of the data. KNN is easy to implement and can handle both categorical and numerical data.\n",
    "\n",
    "- `Logistic Regression`: Logistic Regression is a type of supervised learning algorithm that is mostly used for classification problems. It works by modeling the probability of a binary outcome (e.g., 0 or 1) as a function of the input features. The output of the logistic regression model is a probability score between 0 and 1, which can be interpreted as the likelihood of the binary outcome. Logistic regression is a parametric algorithm, which means that it makes assumptions about the underlying distribution of the data. Logistic regression is easy to interpret and can handle both categorical and numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.KNearestNeighbor = None\n",
    "        self.DecisionTree = None\n",
    "        self.LogisticReg = None\n",
    "        self.RandomForest = None\n",
    "        self.XGBoost = None\n",
    "\n",
    "    \n",
    "    def create_KNearestNeighbor(self, n_neighbors = 8):\n",
    "        self.KNearestNeighbor = self._KNN(self.data, n_neighbors)\n",
    "        return self.KNearestNeighbor\n",
    "\n",
    "    def create_DecisionTree(self, max_depth=8, min_samples_split=8, min_samples_leaf=2, max_leaf_nodes=None):\n",
    "        self.DecisionTree = self._DT(self.data, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "        return self.DecisionTree\n",
    "\n",
    "    def create_LogisticReg(self):\n",
    "        self.LogisticReg = self._LR(self.data)\n",
    "        return self.LogisticReg\n",
    "\n",
    "    def create_RandomForest(self, n_estimators = 10, max_depth = 8, min_samples_split = 5,\n",
    "                             min_samples_leaf = 2, max_leaf_nodes = None):\n",
    "        self.RandomForest = self._RF(self.data, n_estimators, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "        return self.RandomForest\n",
    "\n",
    "    def create_XGBoost(self,learning_rate = 0.1, max_depth = 8, n_estimators = 20):\n",
    "        self.XGBoost = self._XGB(self.data, learning_rate, max_depth, n_estimators)\n",
    "        return self.XGBoost\n",
    "\n",
    "    class _KNN:\n",
    "        def __init__(self, data, n_neighbors):\n",
    "            self.data = deepcopy(data)\n",
    "            self.n_neighbors = n_neighbors\n",
    "\n",
    "            self.knn = KNeighborsClassifier(n_neighbors = self.n_neighbors)\n",
    "            self.knn.fit(self.data.X_train, self.data.y_train)\n",
    "            self.predict = self.knn.predict(self.data.X_test)\n",
    "\n",
    "        def accuracy(self):\n",
    "            return metrics.accuracy_score(self.data.y_test, self.predict)\n",
    "\n",
    "        def train_accuracy(self):\n",
    "            train_predict = self.knn.predict(self.data.X_train)\n",
    "            return metrics.accuracy_score(self.data.y_train, train_predict)\n",
    "\n",
    "        def confusion_matrix(self):\n",
    "            matrix_knear = metrics.confusion_matrix(self.data.y_test, self.predict)\n",
    "            matrix_disp = metrics.ConfusionMatrixDisplay(matrix_knear)\n",
    "            matrix_disp.plot(cmap='Blues')\n",
    "            plt.title('K Nearest Neighbors Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        def grid_search(self):\n",
    "            variables_diverse = {\n",
    "                'n_neighbors': range(1, 20)\n",
    "            }\n",
    "            grid = GridSearchCV(self.knn, variables_diverse, scoring='accuracy', n_jobs=2)\n",
    "            grid.fit(self.data.X_train, self.data.y_train)\n",
    "            score = grid.score(self.data.X_test, self.data.y_test)\n",
    "            return score, grid\n",
    "\n",
    "\n",
    "\n",
    "    class _DT:\n",
    "        def __init__(self, data, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes):\n",
    "            self.data = deepcopy(data)\n",
    "       \n",
    "            self.dt = DecisionTreeClassifier(criterion='entropy', max_depth = max_depth, min_samples_split = min_samples_split,\n",
    "                                             min_samples_leaf = min_samples_leaf, max_leaf_nodes = max_leaf_nodes, random_state = 1)\n",
    "            self.dt.fit(self.data.X_train, self.data.y_train)\n",
    "            self.predict = self.dt.predict(self.data.X_test)\n",
    "\n",
    "        def accuracy(self):\n",
    "            return metrics.accuracy_score(self.data.y_test, self.predict)\n",
    "\n",
    "        def train_accuracy(self):\n",
    "            train_predict = self.dt.predict(self.data.X_train)\n",
    "            return metrics.accuracy_score(self.data.y_train, train_predict)\n",
    "\n",
    "        def confusion_matrix(self):\n",
    "            matrix = metrics.confusion_matrix(self.data.y_test, self.predict)\n",
    "            display_matrix = metrics.ConfusionMatrixDisplay(matrix)\n",
    "            display_matrix.plot(cmap='Blues')\n",
    "            plt.title('Decision Tree Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        def grid_search(self):\n",
    "            variables_diverse = {\n",
    "                'criterion': ['entropy'],\n",
    "                'max_depth': range(2, 10),\n",
    "                'min_samples_split': range(2, 10),\n",
    "                'min_samples_leaf': range(1, 5),\n",
    "                'random_state': [1]\n",
    "            }\n",
    "            grid = GridSearchCV(self.dt, variables_diverse, scoring='accuracy', n_jobs=2)\n",
    "            grid.fit(self.data.X_train, self.data.y_train)\n",
    "            score = grid.score(self.data.X_test, self.data.y_test)\n",
    "            return score, grid\n",
    "\n",
    "\n",
    "\n",
    "    class _LR:\n",
    "        def __init__(self, data):\n",
    "            self.data = deepcopy(data) \n",
    "\n",
    "            self.logreg = LogisticRegression(random_state = 1, solver='lbfgs', max_iter=1000)\n",
    "            self.logreg.fit(self.data.X_train, self.data.y_train)\n",
    "            self.predict = self.logreg.predict(self.data.X_test)\n",
    "\n",
    "        def accuracy(self):\n",
    "            return metrics.accuracy_score(self.data.y_test, self.predict)\n",
    "\n",
    "        def train_accuracy(self):\n",
    "            train_predict = self.logreg.predict(self.data.X_train)\n",
    "            return metrics.accuracy_score(self.data.y_train, train_predict)\n",
    "\n",
    "        def confusion_matrix(self):\n",
    "            matrix = metrics.confusion_matrix(self.data.y_test, self.predict)\n",
    "            display_matrix = metrics.ConfusionMatrixDisplay(matrix)\n",
    "            display_matrix.plot(cmap='Blues')\n",
    "            plt.title('Logistic Regression Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        def grid_search(self):\n",
    "            variables_diverse = {\n",
    "                'penalty' : ['l1','l2'], \n",
    "            }\n",
    "            grid = GridSearchCV(self.logreg, variables_diverse, scoring='accuracy', n_jobs=2)\n",
    "            grid.fit(self.data.X_train, self.data.y_train)\n",
    "            test_score = grid.score(self.data.X_test, self.data.y_test)\n",
    "            return test_score, grid\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class _RF:\n",
    "        def __init__(self, data, n_estimators, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes):\n",
    "            self.data = deepcopy(data)\n",
    "            self.n_estimators = n_estimators\n",
    "            self.max_depth = max_depth\n",
    " \n",
    "            self.forest = RandomForestClassifier(criterion='entropy',n_estimators = n_estimators, max_depth = max_depth,\n",
    "                                                 min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf,\n",
    "                                                 max_leaf_nodes = max_leaf_nodes, random_state = 1)\n",
    "                                                 \n",
    "            self.forest.fit(self.data.X_train, self.data.y_train)\n",
    "            self.predict = self.forest.predict(self.data.X_test)\n",
    "\n",
    "        def accuracy(self):\n",
    "            return metrics.accuracy_score(self.data.y_test, self.predict)\n",
    "\n",
    "        def accuracy_train(self):\n",
    "            train_predict = self.forest.predict(self.data.X_train)\n",
    "            return metrics.accuracy_score(self.data.y_train, train_predict)\n",
    "\n",
    "        def confusion_matrix(self):\n",
    "            matrix = metrics.confusion_matrix(self.data.y_test, self.predict)\n",
    "            display_matrix = metrics.ConfusionMatrixDisplay(matrix)\n",
    "            display_matrix.plot(cmap='Blues')\n",
    "            plt.title('Random Forest Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        def grid_search(self):\n",
    "            variables_diverse = {\n",
    "                'n_estimators': range(1, 20),\n",
    "                'criterion': ['entropy'],\n",
    "                'max_depth': range(1, 10),\n",
    "                'random_state': [1]\n",
    "            }\n",
    "            grid = GridSearchCV(self.forest, variables_diverse, scoring='accuracy', n_jobs=2)\n",
    "            grid.fit(self.data.X_train, self.data.y_train)\n",
    "            test_score = grid.score(self.data.X_test, self.data.y_test)\n",
    "            return test_score, grid\n",
    "    \n",
    "\n",
    "    class _XGB:\n",
    "        def __init__ (self, data, learning_rate, max_depth, n_estimators):\n",
    "            self.data = deepcopy(data)\n",
    "            self.data.y_train = LabelEncoder().fit_transform(self.data.y_train)\n",
    "            self.data.y_test = LabelEncoder().fit_transform(self.data.y_test)\n",
    "            self.model = xgboost.XGBClassifier(learning_rate = learning_rate, max_depth = max_depth, n_estimators = n_estimators)\n",
    "            self.model.fit(self.data.X_train, self.data.y_train)\n",
    "            self.predict = self.model.predict(self.data.X_test)\n",
    "\n",
    "\n",
    "        def accuracy(self):\n",
    "            return metrics.accuracy_score(self.data.y_test, self.predict)\n",
    "\n",
    "        def accuracy_train(self):\n",
    "            train_predict = self.model.predict(self.data.X_train)\n",
    "            return metrics.accuracy_score(self.data.y_train, train_predict)\n",
    "\n",
    "        def confusion_matrix(self):\n",
    "            matrix = metrics.confusion_matrix(self.data.y_test, self.predict)\n",
    "            display_matrix = metrics.ConfusionMatrixDisplay(matrix)\n",
    "            display_matrix.plot(cmap='Blues')\n",
    "            plt.title('XGboosting Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        def grid_search(self):\n",
    "            variables_diverse = {\n",
    "                'n_estimators': [1, 5, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.5],\n",
    "                'max_depth': [1, 3, 5, 7, 9]\n",
    "            }\n",
    "            grid = GridSearchCV(self.model, variables_diverse, scoring='accuracy', n_jobs=2)\n",
    "            grid.fit(self.data.X_train, self.data.y_train)\n",
    "            test_score = grid.score(self.data.X_test, self.data.y_test)\n",
    "            return test_score, grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepearation \n",
    "\n",
    "price is goal! but we need a cutoff to `classify` it, because this methods are not numeric but categorical.\n",
    "\n",
    "so price `median` is a good cutoff to determine `LOW` and `High` price level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['price'] >= data['price'].median(), 'price_level'] = 'HIGH'\n",
    "data.loc[data['price'] < data['price'].median(), 'price_level'] = 'LOW'\n",
    "\n",
    "clf_data = data.drop(['price'], axis = 1)\n",
    "\n",
    "prepeared_data = Data(clf_data)\n",
    "prepeared_data.split_for_classification('price_level')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Results\n",
    "\n",
    "- `accuracy`\n",
    "- `confusion_matrix`\n",
    "\n",
    "|             |         | **Predicted** | **Results** |\n",
    "| :---------- | :-----: | :-: | :-: |\n",
    "|             | *Value* | *0* | *1* |\n",
    "| **Actual**  | *0*     | TN  | FP  |\n",
    "| **Results** | *1*     | FN  | TP  |\n",
    "\n",
    "- **TN** *(True Negative):* The actual result is false, and the model correctly predicted it as false.\n",
    "- **FP** *(False Positive):* The actual result is false, but the model wrongly predicted it as true.\n",
    "- **FN** *(False Negative):* The actual result is true, but the model wrongly predicted it as false.\n",
    "- **TP** *(True Positive):* The actual result is true, and the model correctly predicted it as true.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = Classify(prepeared_data)\n",
    "\n",
    "\n",
    "dt = classification.create_DecisionTree()\n",
    "print(\"Decision Tree:\")\n",
    "print('accuracy =', dt.accuracy())\n",
    "dt.confusion_matrix()\n",
    "\n",
    "knn = classification.create_KNearestNeighbor()\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print('accuracy =', knn.accuracy())\n",
    "knn.confusion_matrix()\n",
    "\n",
    "lr = classification.create_LogisticReg()\n",
    "print(\"Logistic Regression:\")\n",
    "print('accuracy =', lr.accuracy())\n",
    "lr.confusion_matrix()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Overfitting and Underfitting\n",
    "\n",
    "- `Overfitting` occurs when a model is too complex and fits the training data too closely. This means that the model has learned the noise in the training data, rather than the underlying patterns, and is not able to generalize well to new, unseen data. Overfitting can occur when the model has too many parameters relative to the amount of training data, or when the model is trained for too many iterations. Overfitting can be detected by evaluating the performance of the model on a separate test set, and comparing it to the performance on the training set. If the performance on the test set is significantly worse than the performance on the training set, then the model is likely overfitting.\n",
    "\n",
    "- `Underfitting` occurs when a model is too simple and is not able to capture the underlying patterns in the data. This means that the model is not able to fit the training data well, and is not able to generalize well to new, unseen data. Underfitting can occur when the model has too few parameters relative to the complexity of the data, or when the model is trained for too few iterations. Underfitting can be detected by evaluating the performance of the model on the training set, and comparing it to the performance on the test set. If the performance on both the training set and the test set is poor, then the model is likely underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree:\")\n",
    "print('test accuracy =', dt.accuracy())\n",
    "print('train accuracy =', dt.train_accuracy(),'\\n')\n",
    "\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print('test accuracy =', knn.accuracy())\n",
    "print('train accuracy =', knn.train_accuracy(),'\\n')\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print('test accuracy =', lr.accuracy())\n",
    "print('train accuracy =', lr.train_accuracy(),'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to results, train and test result are close so there is no underfitting or overfitting, it is because hyperparameters are chosen wisly( after multiple tests :) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Grid Search\n",
    "\n",
    "Grid search is a technique used in machine learning to find the optimal hyperparameters for a given model. Hyperparameters are parameters that are not learned from the data, but are set by the user before training the model. Examples of hyperparameters include the learning rate, regularization strength, number of hidden layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1 Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, grid = dt.grid_search()\n",
    "print('best grid test reult =', score)\n",
    "grid_result = grid.cv_results_\n",
    "results = pd.DataFrame(grid_result)\n",
    "results.drop(['mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "'std_score_time','split0_test_score','split1_test_score', 'split2_test_score',\n",
    "'split3_test_score','split4_test_score','std_test_score', 'params', 'rank_test_score'], axis=1, inplace=True)\n",
    "results.sort_values('mean_test_score', ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, grid = knn.grid_search()\n",
    "print('best grid test reult =', score)\n",
    "grid_result = grid.cv_results_\n",
    "results = pd.DataFrame(grid_result)\n",
    "results.drop(['mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "'std_score_time','split0_test_score','split1_test_score', 'split2_test_score',\n",
    "'split3_test_score','split4_test_score','std_test_score', 'params', 'rank_test_score'], axis=1, inplace=True)\n",
    "results.sort_values('mean_test_score', ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Effects of preprocess\n",
    "- K-nearest neighbor is strongly depend on normalizing\n",
    "- useless features reduce accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemle learning\n",
    "\n",
    "- `Random forest` is a type of ensemble learning algorithm that is used for both classification and regression problems. It works by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "- The random forest algorithm works by randomly selecting a subset of the input features and a subset of the training data for each decision tree. This process is repeated for each decision tree in the forest, resulting in a set of diverse decision trees that are trained on different subsets of the data. The output of the random forest algorithm is the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "- The random forest algorithm has several advantages over a single decision tree. First, it is less prone to overfitting than a single decision tree, because the diversity of the decision trees in the forest reduces the risk of learning the noise in the data. Second, it can handle both categorical and numerical data, and can handle missing values and outliers. Third, it can provide estimates of feature importance, which can be used to identify the most important features for the classification or regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = classification.create_RandomForest()\n",
    "print(\"Random Forest:\")\n",
    "print('accuracy =', rf.accuracy())\n",
    "rf.confusion_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Bias and Variance\n",
    "- `Bias` refers to the difference between the expected value of the predictions made by a machine learning model and the true values of the target variable. A model with high bias is one that is too simple and is not able to capture the underlying patterns in the data. This means that the model is not able to fit the training data well, and is not able to generalize well to new, unseen data. A model with high bias is said to be underfitting the data.\n",
    "\n",
    "- `Variance` refers to the variability of the predictions made by a machine learning model for different training sets. A model with high variance is one that is too complex and fits the training data too closely. This means that the model has learned the noise in the training data, rather than the underlying patterns, and is not able to generalize well to new, unseen data. A model with high variance is said to be overfitting the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Hyperparameters Effect\n",
    "- `n_estimators`: The number of trees in the forest. Increasing this parameter can improve the performance of the model, but can also increase the risk of overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of each tree in the forest. Increasing this parameter can improve the performance of the model, but can also increase the risk of overfitting.\n",
    "\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node. Increasing this parameter can lead to more conservative tree growth, which can improve the generalization of the model.\n",
    "\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. Increasing this parameter can lead to more conservative tree growth, which can improve the generalization of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_DT_effect(val_range):\n",
    "\n",
    "    accuracy = []\n",
    "    for i in val_range:\n",
    "        randforest = classification.create_RandomForest(n_estimators = i)\n",
    "        accuracy.append(randforest.accuracy())\n",
    "\n",
    "    plt.plot(val_range, accuracy)\n",
    "    plt.xlabel('number_of_DT')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "def max_depth_effects(val_range):\n",
    "\n",
    "    accuracy = []\n",
    "    for i in val_range:\n",
    "        randforest = classification.create_RandomForest(max_depth = i)\n",
    "        accuracy.append(randforest.accuracy())\n",
    "\n",
    "    plt.plot(val_range, accuracy)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "def min_samples_split_effect(val_range):\n",
    "  \n",
    "    accuracy = []\n",
    "    for i in val_range:\n",
    "        randforest = classification.create_RandomForest(min_samples_split = i)\n",
    "        accuracy.append(randforest.accuracy())\n",
    "\n",
    "    plt.plot(val_range, accuracy)\n",
    "    plt.xlabel('min samples split')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "def min_samples_leaf_effect(val_range):\n",
    "  \n",
    "    accuracy = []\n",
    "    for i in val_range:\n",
    "        randforest = classification.create_RandomForest(min_samples_leaf = i)\n",
    "        accuracy.append(randforest.accuracy())\n",
    "\n",
    "    plt.plot(val_range, accuracy)\n",
    "    plt.xlabel('min samples leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "def max_leaf_nodes_effect(val_range):\n",
    "  \n",
    "    accuracy = []\n",
    "    for i in val_range:\n",
    "        randforest = classification.create_RandomForest(max_leaf_nodes = i)\n",
    "        accuracy.append(randforest.accuracy())\n",
    "\n",
    "    plt.plot(val_range, accuracy)\n",
    "    plt.xlabel('max leaf nodes')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_DT_effect(range(1,15))\n",
    "max_depth_effects(range(1,15))\n",
    "min_samples_split_effect(range(2,15))\n",
    "min_samples_leaf_effect(range(1,15))\n",
    "max_leaf_nodes_effect(range(2,15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. X-Gradient Boosting\n",
    "\n",
    "- The `gradient boosting` algorithm works by iteratively adding decision trees to the ensemble, with each new tree attempting to correct the errors of the previous trees. The algorithm starts by fitting a simple model (e.g., a decision tree) to the data, and then calculates the residuals (i.e., the difference between the predicted values and the true values) of the model. The next model is then trained on the residuals of the previous model, with the goal of reducing the residuals and improving the overall performance of the ensemble. This process is repeated for a fixed number of iterations, or until the performance on the validation set stops improving.\n",
    "\n",
    "\n",
    "- The gradient boosting algorithm uses a technique called gradient descent to optimize the parameters of the weak learners. Gradient descent is an iterative optimization algorithm that works by computing the gradient of the loss function with respect to the parameters, and then updating the parameters in the direction of the negative gradient. This process is repeated until the loss function reaches a minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = classification.create_XGBoost()\n",
    "print(\"XGBoosting:\")\n",
    "print('accuracy =', xgb.accuracy())\n",
    "xgb.confusion_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Grid Search\n",
    "\n",
    "- `n_estimators`: The number of trees in the ensemble. Increasing this parameter can improve the performance of the model, but can also increase the risk of overfitting.\n",
    "\n",
    "- `learning_rate`: The step size used in the gradient descent algorithm. A smaller learning rate can lead to slower convergence, but can also improve the generalization of the model.\n",
    "\n",
    "- `max_depth` : The maximum depth of each tree in the ensemble. Increasing this parameter can improve the performance of the model, but can also increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, grid = xgb.grid_search()\n",
    "print('best grid test reult =', score)\n",
    "grid_result = grid.cv_results_\n",
    "results = pd.DataFrame(grid_result)\n",
    "results.drop(['mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "'std_score_time','split0_test_score','split1_test_score', 'split2_test_score',\n",
    "'split3_test_score','split4_test_score','std_test_score', 'params', 'rank_test_score'], axis=1, inplace=True)\n",
    "results.sort_values('mean_test_score', ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Boosting Tree vs Decision Tree\n",
    "- `Ensemble vs. Single Model`: Boosting tree is an ensemble learning algorithm that combines multiple weak learners (usually decision trees) into a strong learner, while decision tree is a single model that makes predictions based on a single tree-like structure.\n",
    "\n",
    "- `Iterative vs. Non-Iterative`: Boosting tree is an iterative algorithm that adds new trees to the ensemble iteratively, with each new tree attempting to correct the errors of the previous trees, while decision tree is a non-iterative algorithm that constructs a single tree-like structure based on the input features.\n",
    "\n",
    "- `Bias-Variance Tradeoff`: Boosting tree is designed to reduce both bias and variance, by combining multiple weak learners into a strong learner that can make accurate predictions on new, unseen data, while decision tree is prone to overfitting, especially when the tree is too deep or the number of training samples is too small.\n",
    "\n",
    "- `Hyperparameters`: Boosting tree has several hyperparameters that can be tuned to improve its performance, such as the learning rate, the number of trees in the ensemble, the maximum depth of each tree, and the minimum number of samples required to split a node, while decision tree has fewer hyperparameters, such as the maximum depth of the tree, the minimum number of samples required to split a node, and the splitting criterion.\n",
    "\n",
    "- `Interpretability`: Decision tree is more interpretable than boosting tree, because it produces a single tree-like structure that can be easily visualized and understood, while boosting tree produces an ensemble of trees that can be more difficult to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
